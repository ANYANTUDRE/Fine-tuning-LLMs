{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Finetuning Phi-2**","metadata":{}},{"cell_type":"markdown","source":"# I. Librairies\n\nImportant imports explained and example of basic usage:\n\n#### datasets\n- **Purpose**: library to easily load and process datasets from HF.\n- **Basic Usage**:\n  ```python\n  from datasets import load_dataset, load_from_disk\n  dataset = load_dataset('path/to/dataset', split='train')\n  ```\n\n#### peft\n- **Purpose**: provides utilities for parameter-efficient fine-tuning.\n- **Basic Usage**:\n  ```python\n  from peft import LoraConfig, prepare_model_for_kbit_training\n  lora_config = LoraConfig()\n  ```\n\n#### transformers\n- **Purpose**: provides classes and functions for transformer models.\n- **Basic Usage**:\n  ```python\n  from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n  model = AutoModelForCausalLM.from_pretrained('model_name')\n  tokenizer = AutoTokenizer.from_pretrained('model_name')\n  ```\n\n#### trl\n- **Purpose**: provides utilities for training transformer models with reinforcement learning.\n- **Basic Usage**:\n  ```python\n  from trl import SFTTrainer\n  trainer = SFTTrainer(model, tokenizer)\n  ```","metadata":{}},{"cell_type":"code","source":"!pip install -q torch peft bitsandbytes scipy trl transformers accelerate einops tqdm huggingface_hub --use-deprecated=legacy-resolver","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T11:01:34.069202Z","iopub.execute_input":"2024-07-05T11:01:34.070205Z","iopub.status.idle":"2024-07-05T11:01:54.780869Z","shell.execute_reply.started":"2024-07-05T11:01:34.070160Z","shell.execute_reply":"2024-07-05T11:01:54.779899Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you'll have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#from dataclasses import dataclass, fields\n#from typing import Optional","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:01:54.782791Z","iopub.execute_input":"2024-07-05T11:01:54.783093Z","iopub.status.idle":"2024-07-05T11:01:54.787121Z","shell.execute_reply.started":"2024-07-05T11:01:54.783064Z","shell.execute_reply":"2024-07-05T11:01:54.786184Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom datasets import load_dataset, load_from_disk\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments \nfrom trl import SFTTrainer\nfrom huggingface_hub import notebook_login\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:49:36.564290Z","iopub.execute_input":"2024-07-05T11:49:36.565189Z","iopub.status.idle":"2024-07-05T11:49:36.572265Z","shell.execute_reply.started":"2024-07-05T11:49:36.565157Z","shell.execute_reply":"2024-07-05T11:49:36.570833Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print(f\"pytorch version {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:02:13.810054Z","iopub.execute_input":"2024-07-05T11:02:13.810809Z","iopub.status.idle":"2024-07-05T11:02:13.816162Z","shell.execute_reply.started":"2024-07-05T11:02:13.810773Z","shell.execute_reply":"2024-07-05T11:02:13.815069Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"pytorch version 2.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"working on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:02:13.819814Z","iopub.execute_input":"2024-07-05T11:02:13.820160Z","iopub.status.idle":"2024-07-05T11:02:13.854650Z","shell.execute_reply.started":"2024-07-05T11:02:13.820133Z","shell.execute_reply":"2024-07-05T11:02:13.853714Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"working on cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"### log in to Hugging Face account\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:02:13.855830Z","iopub.execute_input":"2024-07-05T11:02:13.856182Z","iopub.status.idle":"2024-07-05T11:02:13.880232Z","shell.execute_reply.started":"2024-07-05T11:02:13.856152Z","shell.execute_reply":"2024-07-05T11:02:13.879234Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a8d1ef54a064ab2862d062ff5a888b6"}},"metadata":{}}]},{"cell_type":"markdown","source":"# II. Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"Amod/mental_health_counseling_conversations\", split=\"train\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:49:48.108296Z","iopub.execute_input":"2024-07-05T11:49:48.108682Z","iopub.status.idle":"2024-07-05T11:49:50.655821Z","shell.execute_reply.started":"2024-07-05T11:49:48.108653Z","shell.execute_reply":"2024-07-05T11:49:50.654834Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Context', 'Response'],\n    num_rows: 3512\n})"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.DataFrame(dataset)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:49:53.529410Z","iopub.execute_input":"2024-07-05T11:49:53.529783Z","iopub.status.idle":"2024-07-05T11:49:53.678571Z","shell.execute_reply.started":"2024-07-05T11:49:53.529755Z","shell.execute_reply":"2024-07-05T11:49:53.677291Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                             Context  \\\n0  I'm going through some things with my feelings...   \n1  I'm going through some things with my feelings...   \n2  I'm going through some things with my feelings...   \n3  I'm going through some things with my feelings...   \n4  I'm going through some things with my feelings...   \n\n                                            Response  \n0  If everyone thinks you're worthless, then mayb...  \n1  Hello, and thank you for your question and see...  \n2  First thing I'd suggest is getting the sleep y...  \n3  Therapy is essential for those that are feelin...  \n4  I first want to let you know that you are not ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>If everyone thinks you're worthless, then mayb...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>Hello, and thank you for your question and see...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>First thing I'd suggest is getting the sleep y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>Therapy is essential for those that are feelin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>I first want to let you know that you are not ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:49:56.224779Z","iopub.execute_input":"2024-07-05T11:49:56.225151Z","iopub.status.idle":"2024-07-05T11:49:56.240409Z","shell.execute_reply.started":"2024-07-05T11:49:56.225123Z","shell.execute_reply":"2024-07-05T11:49:56.239217Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3512 entries, 0 to 3511\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Context   3512 non-null   object\n 1   Response  3512 non-null   object\ndtypes: object(2)\nmemory usage: 55.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"def format_row(row):\n    question = row[\"Context\"]\n    response = row[\"Response\"]\n    formatted_string = f\"[INST] {question} [/INST] {response}\"\n    return formatted_string\n\ndf[\"Text\"] = df.apply(format_row, axis=1)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:49:57.541936Z","iopub.execute_input":"2024-07-05T11:49:57.542641Z","iopub.status.idle":"2024-07-05T11:49:57.611353Z","shell.execute_reply.started":"2024-07-05T11:49:57.542605Z","shell.execute_reply":"2024-07-05T11:49:57.610402Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                             Context  \\\n0  I'm going through some things with my feelings...   \n1  I'm going through some things with my feelings...   \n2  I'm going through some things with my feelings...   \n\n                                            Response  \\\n0  If everyone thinks you're worthless, then mayb...   \n1  Hello, and thank you for your question and see...   \n2  First thing I'd suggest is getting the sleep y...   \n\n                                                Text  \n0  [INST] I'm going through some things with my f...  \n1  [INST] I'm going through some things with my f...  \n2  [INST] I'm going through some things with my f...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Context</th>\n      <th>Response</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>If everyone thinks you're worthless, then mayb...</td>\n      <td>[INST] I'm going through some things with my f...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>Hello, and thank you for your question and see...</td>\n      <td>[INST] I'm going through some things with my f...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I'm going through some things with my feelings...</td>\n      <td>First thing I'd suggest is getting the sleep y...</td>\n      <td>[INST] I'm going through some things with my f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_df = df[[\"Text\"]]\ntrain, test = train_test_split(new_df, test_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:49:59.395091Z","iopub.execute_input":"2024-07-05T11:49:59.395949Z","iopub.status.idle":"2024-07-05T11:49:59.407297Z","shell.execute_reply.started":"2024-07-05T11:49:59.395916Z","shell.execute_reply":"2024-07-05T11:49:59.406155Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train.to_csv(\"train_data.csv\", index=False)\ntest.to_csv(\"test_data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:50:15.848840Z","iopub.execute_input":"2024-07-05T11:50:15.849180Z","iopub.status.idle":"2024-07-05T11:50:16.051978Z","shell.execute_reply.started":"2024-07-05T11:50:15.849155Z","shell.execute_reply":"2024-07-05T11:50:16.050963Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train_dataset = load_dataset(\"csv\", data_files=\"train_data.csv\", split=\"train\")\ntest_dataset  = load_dataset(\"csv\", data_files=\"test_data.csv\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:50:43.328545Z","iopub.execute_input":"2024-07-05T11:50:43.328901Z","iopub.status.idle":"2024-07-05T11:50:44.567355Z","shell.execute_reply.started":"2024-07-05T11:50:43.328873Z","shell.execute_reply":"2024-07-05T11:50:44.566227Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"training_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:51:01.207483Z","iopub.execute_input":"2024-07-05T11:51:01.207841Z","iopub.status.idle":"2024-07-05T11:51:01.215609Z","shell.execute_reply.started":"2024-07-05T11:51:01.207813Z","shell.execute_reply":"2024-07-05T11:51:01.214686Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Text'],\n    num_rows: 3512\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# II. Training","metadata":{}},{"cell_type":"code","source":"base_model = \"microsoft/phi-2\"\nnew_model  = \"phi2-ft-mental-health\"","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:02:19.643572Z","iopub.execute_input":"2024-07-05T11:02:19.644012Z","iopub.status.idle":"2024-07-05T11:02:19.648410Z","shell.execute_reply.started":"2024-07-05T11:02:19.643978Z","shell.execute_reply":"2024-07-05T11:02:19.647408Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer\n\n1. **Loading the Tokenizer** (`AutoTokenizer.from_pretrained`): loads a pre-trained tokenizer for the specified model.\n     - **Parameters**:\n       - `base_model`: identifier of the pre-trained model (e.g., a model name or path).\n       - `use_fast=True`: specifies that the fast version of the tokenizer should be used. Fast tokenizers are generally more efficient and quicker.  \n\n\n2. **Setting the Padding Token**(`tokenizer.pad_token = tokenizer.eos_token`): sets the padding token of the tokenizer to be the same as the end-of-sequence (EOS) token. In some models and training setups, it is useful to use the EOS token for padding purposes to ensure consistency in the tokenization process.\n\n3. **Specifying the Padding Side**(`tokenizer.padding_side = \"right\"`): specifies which side of the sequence the padding tokens should be added to.\n   - **Options**:\n     - `\"right\"`: Padding tokens are added to the right side of the sequence.\n     - `\"left\"`: Padding tokens are added to the left side of the sequence.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:02:19.652324Z","iopub.execute_input":"2024-07-05T11:02:19.652615Z","iopub.status.idle":"2024-07-05T11:02:22.906387Z","shell.execute_reply.started":"2024-07-05T11:02:19.652592Z","shell.execute_reply":"2024-07-05T11:02:22.905539Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee0d7289f5b641c8bb139d6ecf546e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"471a370ed40941daa229d993dacda10a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6fa26deda754bceb6c3c25712109a73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77097720c0d2437aa340132701c3fe99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97917bf7c93f463ab4566ed311052b41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d3d5b4a5b8241afb8f2286ee2ca2f84"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## QLora configs\n\n\n#### BitsAndBytesConfig\n\n1. **Class**: `BitsAndBytesConfig`\n2. **Parameters**:\n   - `load_in_4bit=True`: to enable or not loading the model in 4-bit precision (saves memory and speed up computations)\n   - `bnb_4bit_quant_type=\"nf4\"`: Specifies the type of quantization for 4-bit precision.\n     - **Options**: \n       - `\"nf4\"`: Non-Floating 4-bit quantization, which is more memory-efficient.\n       - `\"fp4\"`: Floating Point 4-bit quantization, which offers higher precision.\n   - `bnb_4bit_compute_dtype=torch.float16`: defines the data type for computations with 4-bit precision.\n     - **Options**: \n       - `torch.float16`: Use 16-bit floating point for faster computations with less memory usage.\n       - `torch.float32`: Use 32-bit floating point for higher precision but more memory usage.\n   - `bnb_4bit_use_double_quant=False`: determines whether to use double quantization, which applies quantization twice for better precision (`True`: apply double quantization for better precision, `False`: do not)\n\n\n#### LoraConfig\n\n1. **Class**: `LoraConfig`\n2. **Parameters**:\n   - `r=32`: rank of the low-rank matrix in LoRA, controlling the capacity of the adaptation.\n     - **Options**: Integer values (e.g., 4, 8, 16, 32). Higher values increase the capacity but require more computation.\n   - `lora_alpha=64`: a scaling factor for the low-rank updates, affecting the learning rate of the adaptation.\n     - **Options**: Integer values (e.g., 16, 32, 64, 128). Higher values can lead to larger updates.\n   - `lora_dropout=0.05`: The dropout rate applied to the low-rank updates to prevent overfitting.\n     - **Options**: Float values between 0 and 1 (e.g., 0.1, 0.2, 0.5). Higher values increase regularization.\n   - `bias_type=\"none\"`: Specifies how to handle bias terms in the LoRA layers.\n     - **Options**: \n       - `\"none\"`: no bias terms are adapted.\n       - `\"all\"`: all bias terms are adapted.\n       - `\"some\"`: only some bias terms are adapted.\n   - `task_type=\"CAUSAL_LM\"`: type of task LoRA is being used for.\n     - **Options**: \n       - `\"CAUSAL_LM\"`: Causal Language Modeling, for autoregressive tasks.\n       - `\"SEQ2SEQ_LM\"`: Sequence-to-Sequence Language Modeling, for translation or summarization tasks.\n   - `target_modules=[\"Wqkv\", \"fc1\", \"fc2\"]`: model layers where LoRA will be applied.\n     - **Options**: List of layer names (e.g., `[\"Wqkv\"]`, `[\"fc1\"]`, `[\"fc2\"]`). Specific to the architecture of the model being fine-tuned.","metadata":{}},{"cell_type":"code","source":"bnb_configs = BitsAndBytesConfig(   load_in_4bit=True,\n                                    bnb_4bit_quant_type=\"nf4\",\n                                    bnb_4bit_compute_dtype=torch.float16,\n                                    bnb_4bit_use_double_quant=True\n                                )\n\npeft_configs = LoraConfig(  r=16,\n                            lora_alpha=16,\n                            lora_dropout=0.1,\n                            bias=\"none\",\n                            task_type=\"CAUSAL_LM\",\n                            target_modules=[\"Wqkv\", \"fc1\", \"fc2\"]\n                         )","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:02:22.907479Z","iopub.execute_input":"2024-07-05T11:02:22.907728Z","iopub.status.idle":"2024-07-05T11:02:22.914530Z","shell.execute_reply.started":"2024-07-05T11:02:22.907699Z","shell.execute_reply":"2024-07-05T11:02:22.913706Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Model init and configs\n\n\n1. **Initializing the Model** (`AutoModelForCausalLM.from_pretrained`):loads a pre-trained causal language model.\n   - **Parameters**:\n     - `base_model`: identifier of the pre-trained model (e.g., model name or path).\n     - `flash_attn=True`: enables Flash Attention mechanism, which optimizes attention mechanisms to improve performance.\n     - `flash_rotary=True`: enables Flash Rotary mechanism, which enhances rotary embeddings for better representation learning.\n     - `fused_dense=True`: uses fused dense operations, combining multiple operations into a single kernel for efficiency.\n     - `low_cpu_mem_usage=True`: optimizes for low CPU memory usage, reducing memory footprint during model execution.\n     - `device_map={\"\": 0}`: maps devices for model components.\n     - `revision=\"refs/pr/23\"`: specifies a specific revision of the model to load.\n\n2. **Model configs**:\n   - `model.config.use_cache = False`: disables caching of internal computations in the model.\n   - `model.config.pretraining_tp = 1`: sets a specific pretraining task parameter to 1.\n\n3. **Preparing the Model for k-bit training** (`prepare_model_for_kbit_training`): prepares the model for training with k-bit quantization.\n   - **Parameters**:\n     - `model`: The model instance to prepare for training.\n     - `use_gradient_checkpointing=True`: Enables gradient checkpointing for memory efficiency during training.","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n                                base_model,\n                                quantization_config=bnb_configs,\n                                torch_dtype=torch.float16,\n                                trust_remote_code=True,\n                                flash_attn=True,\n                                flash_rotary=True,\n                                fused_dense=True,\n                                low_cpu_mem_usage=True,\n                                device_map=device,\n                                revision=\"refs/pr/23\"\n                                )\n\n#model.to(device)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:02:22.915627Z","iopub.execute_input":"2024-07-05T11:02:22.915959Z","iopub.status.idle":"2024-07-05T11:32:37.133195Z","shell.execute_reply.started":"2024-07-05T11:02:22.915936Z","shell.execute_reply":"2024-07-05T11:32:37.132291Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2c52cf309e24da29fc7b53151b84366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi.py:   0%|          | 0.00/2.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032e41f2096c4251a150f57386d24add"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_phi.py:   0%|          | 0.00/33.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2452447bdfa44d5db97ef8be08d0bfdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cd850a6d5a24fc3934ae119fbe8c93f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a2c7fd6278646bebb1e5fbaae1ebfde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9af609e11794d3890229a5b61d113c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/577M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2a8d70806db4c1c9a489d1c314bbcd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acacc661f9f042539fe89c1e8ad4f557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f267ca0cb01f4e23b626b3f59de69311"}},"metadata":{}},{"name":"stderr","text":"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model training\n\n**Training arguments Parameters** (`TrainingArguments`):\n - `num_train_epochs=1`: number of times the model will be trained on the entire dataset.\n - `per_device_train_batch_size=2`: number of training samples processed simultaneously on each device (GPU or CPU).\n - `gradient_accumulation_steps=32`: number of batches to accumulate gradients before performing a backward pass.\n   - **Purpose**: helps in training with larger effective batch sizes than memory allows, useful when GPU memory is limited.\n   - **Options**: integer values (e.g., 1, 2, 4, 8, etc.).\n - `evaluation_strategy=\"steps\"`: determines when to perform evaluation during training.\n   - **Purpose**: specifies the strategy for evaluating the model during training, based on steps, epochs, or no evaluation.\n   - **Options**: `\"no\"` (no evaluation), `\"steps\"` (evaluate every `eval_steps`), `\"epoch\"` (evaluate at the end of each epoch).\n - `eval_steps=1500`: interval in steps for evaluation if `evaluation_strategy=\"steps\"`.\n - `logging_steps=1500`: interval in steps for logging training metrics to the console or files.\n - `optim=\"paged_adamw_8bit\"`: optimizer type used for training, here using **paged AdamW with 8-bit precision**.\n   - **Options**: depends on the specific implementation and available optimizers.\n - `learning_rate=2e-4`: initial learning rate for the optimizer.\n   - **Purpose**: controls the step size during gradient descent or optimization.\n   - **Options**: float values (e.g., 0.001, 0.0001, etc.).\n - `lr_scheduler_type=\"cosine\"`: type of learning rate scheduler applied during training.\n   - **Purpose**: adjusts the learning rate during training to optimize model convergence.\n   - **Options**: `\"linear\"`, `\"cosine\"`, `\"step\"`, `\"polynomial\"`, etc., depending on the scheduler implementation.\n - `save_steps=1500`: interval in steps to save model checkpoints.\n - `warmup_ratio=0.05`: ratio of total training steps for which the learning rate will be gradually increased.\n   - **Purpose**: prevents the model from diverging during the initial stages of training by slowly increasing the learning rate.\n   - **Options**: float values between 0 and 1 (e.g., 0.1, 0.05, etc.).\n - `weight_decay=0.01`: strength of weight decay regularization applied to the model parameters during optimization.\n   - **Purpose**: helps prevent overfitting by penalizing large weights.\n   - **Options**: float values (e.g., 0.001, 0.01, etc.).\n - `max_steps=-1`: Maximum number of training steps; `-1` indicates unlimited steps.\n   - **Purpose**: limits the number of iterations the model will undergo during training.\n   - **Options**: integer values or `-1` for unlimited training steps.","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(  output_dir=\"./mental_health\",\n                                    num_train_epochs=1,\n                                    per_device_train_batch_size=1,\n                                    gradient_accumulation_steps=4,\n                                  #gradient_checkpointing=True,\n                                    evaluation_strategy=\"steps\",\n                                    eval_steps=10,\n                                    logging_steps=10,\n                                    optim=\"paged_adamw_8bit\",\n                                    learning_rate=2e-4,\n                                    lr_scheduler_type=\"cosine\",\n                                    save_steps=100,\n                                    warmup_ratio=0.05,\n                                    weight_decay=0.01,\n                                    max_steps=-1\n                                ) \n\ntrainer = SFTTrainer(   model=model,\n                        train_dataset=train_dataset,\n                        eval_dataset=test_dataset,\n                        peft_config=peft_configs,\n                        dataset_text_field=\"Text\",\n                        max_seq_length=690,\n                        tokenizer=tokenizer,\n                        args=training_args\n                    )","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:51:58.066979Z","iopub.execute_input":"2024-07-05T11:51:58.067352Z","iopub.status.idle":"2024-07-05T11:52:00.427301Z","shell.execute_reply.started":"2024-07-05T11:51:58.067322Z","shell.execute_reply":"2024-07-05T11:52:00.426234Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2809 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"764a3881dfb346a5940d4bf66219a996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/703 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0368ea20b61a4f9fb747881b42a73795"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T11:52:04.807686Z","iopub.execute_input":"2024-07-05T11:52:04.808340Z","iopub.status.idle":"2024-07-05T20:30:41.368755Z","shell.execute_reply.started":"2024-07-05T11:52:04.808309Z","shell.execute_reply":"2024-07-05T20:30:41.367723Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='702' max='702' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [702/702 8:38:32, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.692400</td>\n      <td>2.593595</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.663000</td>\n      <td>2.488709</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.525000</td>\n      <td>2.401745</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.495800</td>\n      <td>2.382176</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.429600</td>\n      <td>2.370374</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.409600</td>\n      <td>2.358821</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.443200</td>\n      <td>2.353161</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.357200</td>\n      <td>2.345873</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.337400</td>\n      <td>2.341271</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.378600</td>\n      <td>2.334938</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.459300</td>\n      <td>2.329949</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.298700</td>\n      <td>2.324684</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>2.286900</td>\n      <td>2.314860</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.330800</td>\n      <td>2.311915</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.291300</td>\n      <td>2.304938</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.292500</td>\n      <td>2.301600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.365500</td>\n      <td>2.297969</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.351800</td>\n      <td>2.293280</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.353700</td>\n      <td>2.290932</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.354200</td>\n      <td>2.287564</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.374400</td>\n      <td>2.280515</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.285400</td>\n      <td>2.275631</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>2.287400</td>\n      <td>2.273112</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.339700</td>\n      <td>2.269923</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.263600</td>\n      <td>2.268291</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.278300</td>\n      <td>2.262518</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.294800</td>\n      <td>2.260058</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.304100</td>\n      <td>2.258080</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>2.282400</td>\n      <td>2.252991</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.334600</td>\n      <td>2.250850</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>2.203600</td>\n      <td>2.246943</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.299900</td>\n      <td>2.243791</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>2.270600</td>\n      <td>2.238299</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.220300</td>\n      <td>2.235762</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.269700</td>\n      <td>2.231286</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.131700</td>\n      <td>2.228205</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>2.235900</td>\n      <td>2.227299</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>2.252800</td>\n      <td>2.225412</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>2.279900</td>\n      <td>2.222327</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.237900</td>\n      <td>2.218389</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>2.141400</td>\n      <td>2.216317</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>2.175800</td>\n      <td>2.218100</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>2.331300</td>\n      <td>2.214756</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>2.222800</td>\n      <td>2.212827</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.246500</td>\n      <td>2.210645</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>2.132200</td>\n      <td>2.210012</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>2.205600</td>\n      <td>2.209612</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>2.287000</td>\n      <td>2.206997</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>2.187500</td>\n      <td>2.205707</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.283700</td>\n      <td>2.203431</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>2.232200</td>\n      <td>2.201885</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>2.242900</td>\n      <td>2.201053</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>2.196400</td>\n      <td>2.200575</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>2.176500</td>\n      <td>2.199446</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>2.205500</td>\n      <td>2.198190</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>2.215800</td>\n      <td>2.197263</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>2.233200</td>\n      <td>2.196073</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>2.220600</td>\n      <td>2.195295</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>2.228900</td>\n      <td>2.194735</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.110400</td>\n      <td>2.194391</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>2.142300</td>\n      <td>2.194031</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>2.244100</td>\n      <td>2.193669</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>2.157300</td>\n      <td>2.193298</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>2.197300</td>\n      <td>2.193058</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>2.235900</td>\n      <td>2.192943</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>2.247800</td>\n      <td>2.192830</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>2.261700</td>\n      <td>2.192751</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>2.257100</td>\n      <td>2.192718</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>2.216500</td>\n      <td>2.192705</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.229700</td>\n      <td>2.192703</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de0f2285a741451c937451cbdbdc76aa"}},"metadata":{}},{"name":"stderr","text":"You are using a model of type phi to instantiate a model of type phi-msft. This is not supported for all configurations of models and can yield errors.\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nYou are using a model of type phi to instantiate a model of type phi-msft. This is not supported for all configurations of models and can yield errors.\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nYou are using a model of type phi to instantiate a model of type phi-msft. This is not supported for all configurations of models and can yield errors.\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nYou are using a model of type phi to instantiate a model of type phi-msft. This is not supported for all configurations of models and can yield errors.\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nYou are using a model of type phi to instantiate a model of type phi-msft. This is not supported for all configurations of models and can yield errors.\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nYou are using a model of type phi to instantiate a model of type phi-msft. This is not supported for all configurations of models and can yield errors.\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nYou are using a model of type phi to instantiate a model of type phi-msft. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=702, training_loss=2.285884678533614, metrics={'train_runtime': 31115.9408, 'train_samples_per_second': 0.09, 'train_steps_per_second': 0.023, 'total_flos': 1.2790215975936e+16, 'train_loss': 2.285884678533614, 'epoch': 0.9996440014239943})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Push to hub","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"./trained_model\")\nmodel.push_to_hub(\"/phi-2-ft-mental_health\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:30:41.373350Z","iopub.execute_input":"2024-07-05T20:30:41.374243Z","iopub.status.idle":"2024-07-05T20:30:47.642757Z","shell.execute_reply.started":"2024-07-05T20:30:41.374203Z","shell.execute_reply":"2024-07-05T20:30:47.640971Z"},"trusted":true},"execution_count":40,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trained_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphi-2-ft-mental_health\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2663\u001b[0m, in \u001b[0;36mPreTrainedModel.push_to_hub\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags:\n\u001b[1;32m   2662\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tags\n\u001b[0;32m-> 2663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:875\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    873\u001b[0m organization \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 875\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# Create a new empty model card and eventually tag it\u001b[39;00m\n\u001b[1;32m    880\u001b[0m model_card \u001b[38;5;241m=\u001b[39m create_and_tag_model_card(\n\u001b[1;32m    881\u001b[0m     repo_id, tags, token\u001b[38;5;241m=\u001b[39mtoken, ignore_metadata_errors\u001b[38;5;241m=\u001b[39mignore_metadata_errors\n\u001b[1;32m    882\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:691\u001b[0m, in \u001b[0;36mPushToHubMixin._create_repo\u001b[0;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[1;32m    688\u001b[0m             repo_id \u001b[38;5;241m=\u001b[39m repo_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    689\u001b[0m         repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morganization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 691\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m url\u001b[38;5;241m.\u001b[39mrepo_id\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3256\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3256\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3259\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-668857f6-4bd501053fbe542a7ef73c4e;43f76eb4-692b-4cd5-bcfa-844a39c4a9db)\n\nInvalid username or password."],"ename":"HfHubHTTPError","evalue":"401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-668857f6-4bd501053fbe542a7ef73c4e;43f76eb4-692b-4cd5-bcfa-844a39c4a9db)\n\nInvalid username or password.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
